---
title: "Recurrent Neural Network"
subtitle: "Session1"
author: "Gyeongdeok Seo"
date: 2020-05-18T21:13:14-05:00
categories: ["R"]
mainfont: NanumGothic
tags: ["R Markdown", "plot", "regression"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

# Explanation
 CNN과 NN에서는 입출력의 차원이 고정되어 있는 경우가 대부분이었다. RNN에서는 하지만 입출력의 차원이 유동적이다. 주로 시계열자료나 time-dependent data(NL, Audio) 등에 사용이 된다.
 CNN과 NN과 같이 feed-forward network 계열(재귀적인 레이어가 존재하지 않는다.)과는 달리 RNN은 inner memory를 모델에 사용한다. 그리고 입력으로 이전의 출력값과 히든 스테이트를 사용하는 것이 특징이다.
<br>
<center>
![RNN network](/post/2015-07-23-r-rmarkdown_files/rnn1.png){#id .class width="70%"}
<center>

# Equation
 $$ Hidden-nodes : a_t = W_Hh_{t-1} + W_XX_t $$
 이 때 a는 히든레이어의 입력이다. a는 이전시점에서의 히든 스테이트의 아웃풋과 웨이트의 곱 + 현재시점에서의 인풋값과 웨이트의 곱이다.
  $$ h_t = tanh(a_t), y_t=softmax(W_Yh_t) $$
  k: the dimension of input vector x
  d: the number of hidden nodes (hyperparameter)
  $$ W_x \in d\times k, W_Y \in k \times d, W_H \in d\times d $$
  $$ a_t, h_t \in d \times 1, y_t \in k \times 1 $$
  $$ tanh(a_t) = \frac{e^{a_t}-e^{-a_t}}{e^{a_t}+e^{a_t}} $$
  $$ softmax(W_Yh_t) = \frac{e^{W_Yh_t}}{\sum e^{W_Yh_T}} $$
 
 
 
 Remind 

 

# Vanishing Gradient
 Vanishing Gradient 문제가 RNN에서 자주 발생할 때가 있다. 왜냐하면 Time-dependent 모델이기 때문에 특정 스테이트에서 모델을 업데이트 하기 위해서는 과거 스테이트의 정보가 다 들어가기 때문이다. 이런 경우에 작은 값이 있으면 바로 0으로 수렴해가기 때문에 이를 해결할 방법이 필요하다.
 
# LSTM
 그 방법으로 먼저 Long Short Term Memory (LSTM)이 있다. 이 경우 학습 중간에 Forget gate를 넣어서 필요없는 항을 걸러준다. 즉, 중간에 중요성을 판단해준다고 보면 된다.
 
# GRU
 그 다음 Gated Recurrent Unit (GRU)이 있다. 이는 Simplified version of LSTM 이다.

# 마크다운용 주소참고
https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html